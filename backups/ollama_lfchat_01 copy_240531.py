import os
import pandas as pd
import numpy as np
from numpy import dot
from numpy.linalg import norm
# import ast
import streamlit as st
from streamlit_chat import message
from dotenv import load_dotenv
# import time
import streamlit.components.v1 as components
# import random
# import asyncio
# import logging
# import requests 
# import json
# import ollama
from ollama import Client
# import asyncio
# from ollama import AsyncClient
# import requests
from openai import OpenAI

load_dotenv()

st.set_page_config(page_title='L&F Chat ', page_icon=':ğŸ³:', layout="wide")

## title    



st.error('ğŸš¨ ê²€ìƒ‰ì–´ì— ì£¼ì˜í•˜ì„¸ìš”, ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ì€ ì‚¼ê°€í•´ì£¼ì„¸ìš”!')
colored_title = """
    <h1 style="margin-top:20px;">
    <span style="color: white;">WELCOME TO</span><span style="color:#FF3F00;">&nbsp; L&FğŸ”¥</span>
</h1>
"""
if "colored_title" not in st.session_state:
  st.session_state["colored_title"] = colored_title

st.markdown(st.session_state["colored_title"], unsafe_allow_html=True)

with st.sidebar:
    # with st.echo():
    #     st.write("ì‚¬ì´ë“œë°”")

    # with st.spinner("Loading..."):
    #     time.sleep(5)
    st.success('ì£¼ì˜ì‚¬í•­ì— ëŒ€í•œ ì•ˆë‚´ë¥¼ í™•ì¸í•˜ì„¸ìš”. http://lfon.landf.co.kr')
    st.success('ğŸ”¥ ì•„ë˜ëŠ” ê²€ìƒ‰ì–´ ì˜ˆì‹œì…ë‹ˆë‹¤.')
    st.write('<br>[1] mes ë‹´ë‹¹ìê°€ ëˆ„êµ¬ì•¼?<br><br>[2] hr ë‹´ë‹¹ìê°€ ëˆ„êµ¬ì•¼? <br><br> [3] LFON ê³„ì •ì„ ìƒì„±í•˜ê³  ì‹¶ì–´.', unsafe_allow_html=True)


NAME = 'ê¹€ìš©êµ­(wg0403)' #CONST
res_message = 'test'

# Set OpenAI API key
clientEmbedding = OpenAI(
    api_key=os.environ.get('OPENAI_API_KEY'),
)

def prompt_clear():
    st.session_state["generated"] = []
    st.session_state['past'] = []

def get_embedding(text, engine): # ì…ë ¥ë°›ì€ ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì™€, ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ ì„ë² ë”©ì„ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜.
    text = text.replace("\n", " ")
    return clientEmbedding.embeddings.create(input = [text], model=engine).data[0].embedding

# ë‘ ì„ë² ë”©ê°„ ìœ ì‚¬ë„ ê³„ì‚°
def cos_sim(A, B):
    return dot(A, B)  / (norm(A)*norm(B))

# ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ê³ , ìœ ì‚¬ë„ ë†’ì€ íƒ‘3 ìë£Œ
def return_answer_candidate(df, query):
    query_embedding = get_embedding(
        query,  
        engine="text-embedding-ada-002"
    )
    # ì…ë ¥ëœ ì§ˆë¬¸ê³¼ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„
    df['similarity'] = df['embedding'].apply(lambda x: cos_sim(np.array(query_embedding), np.array(x)))
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    top2 = df.sort_values("similarity", ascending=False).head(3)
    return top2


# ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ3ê°œ ê°€ì ¸ì™€ì„œ, messagesì…‹ ë§Œë“¤ì–´ì„œ ë¦¬í„´    
def create_prompt(query):

    file_list = os.listdir(folder_path)

    all_data = []   

    for file_name in file_list:
        if file_name.endswith('.csv'):
            file_path = os.path.join(folder_path, file_name)
            df = pd.read_csv(file_path, encoding='utf-8')
            df['embedding'] = df['embedding'].apply(lambda x: [float(num) for num in x.strip('[]').split(',')])
            all_data.append(df)

    combined_df = pd.concat(all_data, ignore_index=True)

    # ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œ ê°€ì ¸ì˜¤ê¸°
    result = return_answer_candidate(combined_df, query)
    print('###ë°ì´í„°í”„ë ˆì„###')
    print(result) # ê²°ê³¼ DataFrame ì¶œë ¥

    
    # ë¬¸ì„œ ë‚´ìš©ì„ ë™ì ìœ¼ë¡œ í¬ë§·íŒ…í•˜ê¸° ìœ„í•œ ì½”ë“œ
    system_message_parts = [
        f"ë¬¸ì„œ{i + 1}: {text}"
        for i, text in enumerate(result['text'])  # result DataFrameì—ì„œ 'text' ì—´ì˜ ëª¨ë“  í•­ëª©ì„ ìˆœíšŒ
    ]

    # ëª¨ë“  ë¬¸ì„œ ë¶€ë¶„ì„ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    documents_formatted = "\n".join(system_message_parts) + "\n"

    print('###ë¬¸ì„œ í¬ë§·###')
    print(documents_formatted)

    # ìµœì¢… system_messageì— ë¬¸ì„œ ë‚´ìš© í¬í•¨
    system_message = f"""
    ë‚˜ëŠ” L&Fë´‡ì…ë‹ˆë‹¤. ë‚˜ëŠ” L&Fì˜ ì‚¬ë‚´ì •ì±…, ë¬¸í™”, ì¡°ì§ì— ëŒ€í•´ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì•„ë˜ ì„¸ ê°œì˜ ì—­ë”°ì˜´í‘œë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ëŠ” ì‚¬ë‚´ ë¬¸ì„œì…ë‹ˆë‹¤.
    ```{documents_formatted}```
    1. ì‚¬ë‚´ ë¬¸ì„œì— ê·¼ê±°í•´ì„œ user content ì§ˆì˜ì— ëŒ€í•´ì„œ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
    2. user content ì§ˆì˜ë‚´ìš©ì´ ì‚¬ë‚´ ë¬¸ì„œì— ì—†ë‹¤ë©´, 'ì£„ì†¡í•©ë‹ˆë‹¤. ê·¸ ë¶€ë¶„ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.' ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    3. ë§í¬ê°€ ìˆë‹¤ë©´ ë‹µë³€ì— ê¼­ í¬í•¨í•˜ì—¬ ì£¼ì„¸ìš”.
    4. ë¬´ì¡°ê±´ ì¡´ëŒ“ë§ë¡œ ë‹µí•˜ì„¸ìš”.
    """

    # system_message = f"""
    # ë„ˆì˜ ì´ë¦„ì€ L&Fë´‡ì´ì•¼. ë„ˆëŠ” L&Fì˜ ì‚¬ë‚´ì •ì±…, ë¬¸í™”, ì¡°ì§ì— ëŒ€í•´ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ì±—ë´‡ì´ì•¼.
    # ë„ˆëŠ” ì£¼ì–´ì§„ ë¬¸ì„œë§Œì„ ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´ì¤˜. ì§ˆë¬¸ê³¼ ë¬¸ì„œì™€ ê´€ë ¨ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹µë³€í•˜ì§€ë§ˆ.
    # """

    # system_message =f"""
    # ë„ˆì˜ ì´ë¦„ì€ L&Fë´‡ì´ì•¼. ë„ˆëŠ” L&Fì˜ ì‚¬ë‚´ì •ì±…, ë¬¸í™”, ì¡°ì§ì— ëŒ€í•´ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ì±—ë´‡ì´ì•¼.
    # í•œê¸€ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜.
    # """
    
    user_message = f"""User question: "{str(query)}". """

    messages =[
        {"role": "user", "content": user_message},
        {"role": "system", "content": system_message}
    ]
    # print(messages)
    return messages


# ì™„ì„±ëœ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
def generate_response(messageArr):
    res_message = []

    client = Client(host='http://192.168.120.253:11434')

    # response = client.chat(model='llama-3-alpha-ko-8b-instruct-q4_k_m', messages=messageArr,
    response = client.chat(model='EEVE-10.8B', messages=messageArr,
        stream = True,
        options = {
            "temperature": 0,
            "top_k":10,
            "top_p":0.5,
            "num_ctx":500,
            "mirostat_tau":1.0,
            "mirostat":1
            # "stop" : ["\n", "----------------------------------------------------"]
        }
    )
    for chunk in response:
        print(chunk['message']['content'], end='', flush=True)
        token = chunk['message']['content']
        res_message += token
        yield chunk['message']['content']
    
# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    avatar = "https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg" if message["role"] == "user" else "ğŸŒ"

    with st.chat_message(message["role"], avatar = avatar):
        st.markdown(message["content"])

#############################################################################
#############################################################################

folder_path = './data'  
# í´ë” ë° ëª¨ë“  íŒŒì¼ ì¤‘ txtíŒŒì¼ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì™€ì„œ for ë¬¸ ì‹œì‘
txt_file_name = [file for file in os.listdir(folder_path) if file.endswith('.txt')]

#### main logic


# React to user input
if user_input := st.chat_input("'LFON ê³„ì •ìƒì„±ì„ í•˜ê³ ì‹¶ì–´', 'ê²½ì˜ì •ë³´íŒ€ì— ëŒ€í•´ ì•Œë ¤ì¤˜'"):
    # Display user message in chat message container
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user", avatar="https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg"):
        st.markdown(user_input)
        
    with st.chat_message("assistant", avatar="ğŸŒ"):
        prompt = create_prompt(user_input)
        response = st.write_stream(generate_response(prompt))
        print('###ì‚¬ìš©ì ì§ˆì˜###')
        print(user_input)

    st.session_state.messages.append({"role": "assistant", "content": message})


