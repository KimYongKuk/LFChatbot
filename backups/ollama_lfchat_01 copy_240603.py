import os
import pandas as pd
import numpy as np
from numpy import dot
from numpy.linalg import norm
# import ast
import streamlit as st
from streamlit_chat import message
from dotenv import load_dotenv
import streamlit.components.v1 as components
from ollama import Client
from openai import OpenAI
from langchain.prompts import ChatPromptTemplate
from sentence_transformers import SentenceTransformer
import torch
import uuid
import chromadb
from chromadb.utils import embedding_functions

from langchain.document_loaders import TextLoader
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# from langchain_chroma import chroma
from langchain_community.embeddings.sentence_transformer import (SentenceTransformerEmbeddings,)

from tqdm import tqdm

load_dotenv()

st.set_page_config(page_title='L&F Chat ', page_icon=':ğŸ³:', layout="wide")

## title    



st.error('ğŸš¨ ê²€ìƒ‰ì–´ì— ì£¼ì˜í•˜ì„¸ìš”, ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ì€ ì‚¼ê°€í•´ì£¼ì„¸ìš”!')
colored_title = """
    <h1 style="margin-top:20px;">
    <span style="color: white;">WELCOME TO</span><span style="color:#FF3F00;">&nbsp; L&F BOTğŸ”¥</span>
</h1>
"""
if "colored_title" not in st.session_state:
  st.session_state["colored_title"] = colored_title

st.markdown(st.session_state["colored_title"], unsafe_allow_html=True)

with st.sidebar:
    # with st.echo():
    #     st.write("ì‚¬ì´ë“œë°”")

    # with st.spinner("Loading..."):
    #     time.sleep(5)
    st.success('ğŸ”¥ ì£¼ì˜ì‚¬í•­ì— ëŒ€í•œ ì•ˆë‚´ë¥¼ í™•ì¸í•˜ì„¸ìš”. http://lfon.landf.co.kr')
    st.markdown('---')
    # st.success('ğŸ”¥ ì•„ë˜ëŠ” ê²€ìƒ‰ì–´ ì˜ˆì‹œì…ë‹ˆë‹¤.')
    st.write('<p style="font-size: 17px;"><span class="bold">ğŸ¤—ì•„ë˜ëŠ” ê²€ìƒ‰ì–´ ì˜ˆì‹œì…ë‹ˆë‹¤ğŸ¤—</span></p> ', unsafe_allow_html=True)

    st.markdown('<p style="font-size: 15px;"> # MES ë‹´ë‹¹ìê°€ ëˆ„êµ¬ì•¼? </p>\
                 <p style="font-size: 15px;"> # LFON ê³„ì •ì„ ìƒì„±í•˜ê³  ì‹¶ì–´. </p>\
                 <p style="font-size: 15px;"> # í¬ë§· í•˜ê³ ì‹¶ì€ë° ì–´ë–»ê²Œí•´ì•¼í•´? </p>\
                 <p style="font-size: 15px;"> # L&Fì˜ ì¡°ì§ë¬¸í™”ì— ëŒ€í•´ ì•Œë ¤ì¤˜. </p>\
                 <p style="font-size: 15px;"> # ì „í‘œ í”„ë¡œì„¸ìŠ¤ê°€ ê¶ê¸ˆí•œë° ì•Œë ¤ì¤„ë˜? </p>', unsafe_allow_html=True)



#CONST
################################
NAME = 'ê¹€ìš©êµ­(wg0403)' 
CHROMADB_PATH   = "./chdata" 
CHUNK_SIZE = 4096
res_message = 'test'
################################

#PersistentClientì˜ ê²½ìš°, ì˜êµ¬ì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥ ê°€ëŠ¥í•¨
#VectorDB Chromaì˜ client ìƒì„±.
perClient = chromadb.PersistentClient(
    path=CHROMADB_PATH
)

#Collection ìƒì„±.
posts = perClient.get_or_create_collection (
    name="lftest01",
    # metadata={'hnsw:space': 'cosine'},
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="bespin-global/klue-sroberta-base-continue-learning-by-mnr")
) #bespin-global/klue-sroberta-base-continue-learning-by-mnr

#ì„ë² ë”© ëª¨ë¸, dimension 768
embeddingModel = SentenceTransformerEmbeddings(model_name="bespin-global/klue-sroberta-base-continue-learning-by-mnr")

#set vectorStore
vectorStore = Chroma(
    client = perClient,
    collection_name="lftest01",
    embedding_function=embeddingModel
)

#return embedding model
def test_embedding(text):
    text = text.replace("\n", " ")
    model = SentenceTransformer("bespin-global/klue-sroberta-base-continue-learning-by-mnr")
    embeddings = model.encode(text)
    return embeddings

def prompt_clear():
    st.session_state["generated"] = []
    st.session_state['past'] = []

def get_embedding(text, engine): # ì…ë ¥ë°›ì€ ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì™€, ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ ì„ë² ë”©ì„ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜.
    text = text.replace("\n", " ")
    return clientEmbedding.embeddings.create(input = [text], model=engine).data[0].embedding

# ë‘ ì„ë² ë”©ê°„ ìœ ì‚¬ë„ ê³„ì‚°
def cos_sim(A, B):
    return dot(A, B)  / (norm(A)*norm(B))

# ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ê³ , ìœ ì‚¬ë„ ë†’ì€ íƒ‘3 ìë£Œ
def return_answer_candidate(df, query):
    query_embedding = get_embedding(
        query,  
        engine="text-embedding-ada-002"
    )

    #model import
    model = SentenceTransformer("bespin-global/klue-sroberta-base-continue-learning-by-mnr")
    # sentences = ["This is an example sentence", "Each sentence is converted"]
    query_embedding2 = model.encode(query)

    print('###1ë²ˆ###')
    print(query_embedding)
    print('###2ë²ˆ###')
    print(query_embedding2)

    print('###3ë²ˆ###')
    query_embedding2_float = [tensor.item() for tensor in query_embedding2]
    print(query_embedding2_float)

    # ì…ë ¥ëœ ì§ˆë¬¸ê³¼ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„
    df['similarity'] = df['embedding'].apply(lambda x: cos_sim(np.array(query_embedding2_float), np.array(x)))
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    top2 = df.sort_values("similarity", ascending=False).head(3)
    return top2

#split text
def split_Value():
    loader = DirectoryLoader('./data', glob="*.txt", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)
    texts = text_splitter.split_documents(documents)
    return texts

#return answer in vectorDB
def answer_vectorDB(query): # query = "ê²½ì˜ì •ë³´íŒ€ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
    docs = vectorStore.similarity_search(query)
    return docs
    # print('ì •ë‹µ1â˜…' + docs[0].page_content)
    # print('#############################')
    # print(vectorStore._collection.count(), "in the collection")


# textíŒŒì¼ì„ ì„ë² ë”© csvë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def create_embedding(txt_file_name, folder_path):
 #split ë¬¸ì„œì •ë³´ë¥¼ Collectionì— add.
    tmpIdx = 0
    for doc in split_Value():
        tmpIdx += 1
        posts.add(
            ids=['ë¬¸ì„œ' + str(tmpIdx)], documents=doc.page_content
        )

    #lftest01 collectionìœ¼ë¡œ vectorstore ìƒì„±
  

    # vectorStore = Chroma.from_documents(
    #     documents = split_Value(),
    #     embedding = embeddingModel,
    #     persist_directory="./data"
    # )

    # perClient.delete_collection("lftest01")
    # print(posts.get())

# ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ3ê°œ ê°€ì ¸ì™€ì„œ, messagesì…‹ ë§Œë“¤ì–´ì„œ ë¦¬í„´    
def create_prompt(query):
    
    # ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œ ê°€ì ¸ì˜¤ê¸°
    result = answer_vectorDB(query)
    print('###ë°ì´í„°í”„ë ˆì„###')
    print(result) # ê²°ê³¼ DataFrame ì¶œë ¥

    
    # ë¬¸ì„œ ë‚´ìš©ì„ ë™ì ìœ¼ë¡œ í¬ë§·íŒ…í•˜ê¸° ìœ„í•œ ì½”ë“œ
    system_message_parts = []
    system_message_parts.extend(doc.page_content for doc in result)

    # ëª¨ë“  ë¬¸ì„œ ë¶€ë¶„ì„ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    documents_formatted = "\n".join(system_message_parts) + "\n"

    print('###ë¬¸ì„œ í¬ë§·###')
    print(documents_formatted)

    # ìµœì¢… system_messageì— ë¬¸ì„œ ë‚´ìš© í¬í•¨
    system_message = f"""
    ë‚˜ëŠ” L&Fë´‡ì…ë‹ˆë‹¤. ë‚˜ëŠ” L&Fì˜ ì‚¬ë‚´ì •ì±…, ë¬¸í™”, ì¡°ì§ì— ëŒ€í•´ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì•„ë˜ ì„¸ ê°œì˜ ì—­ë”°ì˜´í‘œë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ëŠ” ì‚¬ë‚´ ë¬¸ì„œì…ë‹ˆë‹¤.
    ```{documents_formatted}```
    1. ì‚¬ë‚´ ë¬¸ì„œì— ê·¼ê±°í•´ì„œ user content ì§ˆì˜ì— ëŒ€í•´ì„œ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
    2. user content ì§ˆì˜ë‚´ìš©ì´ ì‚¬ë‚´ ë¬¸ì„œì— ì—†ë‹¤ë©´, 'ì£„ì†¡í•©ë‹ˆë‹¤. ê·¸ ë¶€ë¶„ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.' ë¼ê³ ë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    3. ë§í¬ê°€ ìˆë‹¤ë©´ ë‹µë³€ì— ê¼­ í¬í•¨í•˜ì—¬ ì£¼ì„¸ìš”.
    4. ë¬´ì¡°ê±´ í•œê¸€ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
    """

    user_message = f"""User question: "{str(query)}". """

    messages =[
        {"role": "user", "content": user_message},
        {"role": "system", "content": system_message}
    ]
    # print(messages)
    return messages


# ì™„ì„±ëœ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
def generate_response(messageArr):
    res_message = []

    client = Client(host='http://192.168.120.253:11434')

    # response = client.chat(model='llama-3-alpha-ko-8b-instruct-q4_k_m', messages=messageArr,
    response = client.chat(model='gemma', messages=messageArr,
        stream = True,
        options = {
            # "temperature": 0,
            # "top_k":10,
            # "top_p":0.5,
            # "num_ctx":500,
            # "mirostat_tau":1.0,
            # "mirostat":1
            # "stop" : ["\n", "----------------------------------------------------"]
        }
    )
    for chunk in response:
        print(chunk['message']['content'], end='', flush=True)
        token = chunk['message']['content']
        res_message += token
        yield chunk['message']['content']
    
# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    avatar = "https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg" if message["role"] == "user" else "ğŸŒ"

    with st.chat_message(message["role"], avatar = avatar):
        st.markdown(message["content"])

#############################################################################
#############################################################################

folder_path = './data'  
# í´ë” ë° ëª¨ë“  íŒŒì¼ ì¤‘ txtíŒŒì¼ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì™€ì„œ for ë¬¸ ì‹œì‘
txt_file_name = [file for file in os.listdir(folder_path) if file.endswith('.txt')]

# test_embedding('test')
#### main logic
create_embedding(txt_file_name, folder_path)

# React to user input
if user_input := st.chat_input("'LFON ê³„ì •ìƒì„±ì„ í•˜ê³ ì‹¶ì–´', 'ê²½ì˜ì •ë³´íŒ€ì— ëŒ€í•´ ì•Œë ¤ì¤˜'"):
    # Display user message in chat message container
    # st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user", avatar="https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg"):
        st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        
    with st.chat_message("assistant", avatar="ğŸŒ"):
        prompt = create_prompt(user_input)
        response = st.write_stream(generate_response(prompt))
        print('###ì‚¬ìš©ì ì§ˆì˜###')
        print(user_input)
        st.session_state.messages.append({"role": "assistant", "content": response})