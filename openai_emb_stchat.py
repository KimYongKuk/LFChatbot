import os
import openai
from openai import OpenAI
import pandas as pd
import numpy as np
from numpy import dot
from numpy.linalg import norm
import ast
import streamlit as st
from streamlit_chat import message
from dotenv import load_dotenv
import time
import streamlit.components.v1 as components
import random
import asyncio
import logging

load_dotenv()

st.set_page_config(page_title='L&F Chat ', page_icon=':ğŸ³:', layout="wide")


# Set OpenAI API key
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get('OPENAI_API_KEY'),
)

def prompt_clear():
    st.session_state["generated"] = []
    st.session_state['past'] = []

def get_embedding(text, engine): # ì…ë ¥ë°›ì€ ì†ŒìŠ¤ í…ìŠ¤íŠ¸ì™€, ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ ì„ë² ë”©ì„ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜.
    text = text.replace("\n", " ")
    return client.embeddings.create(input = [text], model=engine).data[0].embedding


# ë‘ ì„ë² ë”©ê°„ ìœ ì‚¬ë„ ê³„ì‚°
def cos_sim(A, B):
    return dot(A, B)  / (norm(A)*norm(B))

# ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ê³ , ìœ ì‚¬ë„ ë†’ì€ íƒ‘3 ìë£Œ
def return_answer_candidate(df, query):
    query_embedding = get_embedding(
        query,
        engine="text-embedding-ada-002"
    )
    # ì…ë ¥ëœ ì§ˆë¬¸ê³¼ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„
    df['similarity'] = df['embedding'].apply(lambda x: cos_sim(np.array(query_embedding), np.array(x)))
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    top2 = df.sort_values("similarity", ascending=False).head(2)
    return top2




# ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ3ê°œ ê°€ì ¸ì™€ì„œ, messagesì…‹ ë§Œë“¤ì–´ì„œ ë¦¬í„´
def create_prompt(query):

    file_list = os.listdir(folder_path)

    all_data = []

    for file_name in file_list:
        if file_name.endswith('.csv'):
            file_path = os.path.join(folder_path, file_name)
            df = pd.read_csv(file_path, encoding='utf-8')
            df['embedding'] = df['embedding'].apply(lambda x: [float(num) for num in x.strip('[]').split(',')])
            all_data.append(df)

    combined_df = pd.concat(all_data, ignore_index=True)

    # ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 3ê°œ ê°€ì ¸ì˜¤ê¸°
    result = return_answer_candidate(combined_df, query)
    # print(result) # ê²°ê³¼ DataFrame ì¶œë ¥

    # ë¬¸ì„œ ë‚´ìš©ì„ ë™ì ìœ¼ë¡œ í¬ë§·íŒ…í•˜ê¸° ìœ„í•œ ì½”ë“œ
    system_message_parts = [
        f"ë¬¸ì„œ{i + 1}: {text}"
        for i, text in enumerate(result['text'])  # result DataFrameì—ì„œ 'text' ì—´ì˜ ëª¨ë“  í•­ëª©ì„ ìˆœíšŒ
    ]


    
    # ëª¨ë“  ë¬¸ì„œ ë¶€ë¶„ì„ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    documents_formatted = "\n".join(system_message_parts)

    # ìµœì¢… system_messageì— ë¬¸ì„œ ë‚´ìš© í¬í•¨
    system_message = f"""
    ë„ˆì˜ ì´ë¦„ì€ L&Fë´‡ì´ì•¼. ë„ˆëŠ” L&Fì˜ ì‚¬ë‚´ì •ì±…, ë¬¸í™”, ì¡°ì§ì— ëŒ€í•´ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ì±—ë´‡ì´ì•¼.
    ë„ˆëŠ” ì£¼ì–´ì§„ ë¬¸ì„œë§Œì„ ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´ì¤˜. ì§ˆë¬¸ê³¼ ë¬¸ì„œì™€ ê´€ë ¨ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹µë³€í•˜ì§€ë§ˆ.
    ë¬¸ì„œë‚´ìš©:
    {documents_formatted}
    ë¬¸ì„œì— ë‚˜ì™€ìˆëŠ” ê±´, ì¹œì ˆíˆ ì•Œë ¤ì£¼ë˜, ë¬¸ì„œì— ë‚˜ì™€ìˆì§€ ì•Šì€ ê±´ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•´ì¤˜.
    ë¬´ì¡°ê±´ ì¡´ëŒ“ë§ë¡œ ëŒ€ë‹µí•´ì¤˜.
    """

    user_message = f"""User question: "{str(query)}". """

    messages =[
        {"role": "user", "content": user_message},
        {"role": "system", "content": system_message}
    ]
    # print(messages)
    return messages

# ì™„ì„±ëœ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
def generate_response(messages):
    result = client.chat.completions.create(
            stream=True,
            model="gpt-3.5-turbo",
            messages= messages,
            temperature = 0.5,
            max_tokens = 1000
        )
    # print(result.choices[0].message.content)
    return result #result.choices[0].message.content

#############################################################################
#############################################################################

folder_path = './data'
# í´ë” ë° ëª¨ë“  íŒŒì¼ ì¤‘ txtíŒŒì¼ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì™€ì„œ for ë¬¸ ì‹œì‘
txt_file_name = [file for file in os.listdir(folder_path) if file.endswith('.txt')]

## title

st.error('ğŸš¨ ê²€ìƒ‰ì–´ì— ì£¼ì˜í•˜ì„¸ìš”, ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ì€ ì‚¼ê°€í•´ì£¼ì„¸ìš”!')
colored_title = """
    <h1 style="margin-top:20px;">
    <span style="color: white;">WELCOME TO</span><span style="color:#FF3F00;">&nbsp; L&FğŸ”¥</span>
</h1>
"""
if "colored_title" not in st.session_state:
  st.session_state["colored_title"] = colored_title

st.markdown(st.session_state["colored_title"], unsafe_allow_html=True)

#### main logic
data = []

for file in txt_file_name:
    if os.path.isfile(os.path.join(folder_path, file + '.csv')):
        continue
    
    data = []
    txt_file_path = os.path.join(folder_path, file)
    with open(txt_file_path, 'r', encoding='utf-8') as f:
        file_name = file + '.csv'
        file_path = os.path.join(folder_path, file_name)
        text = f.read()
        data.append(text)
        df = pd.DataFrame(data, columns=['text'])
        df['embedding'] = df.apply(lambda row: get_embedding(row.text, engine="text-embedding-ada-002"), axis=1) # ì„ë² ë”© ë°ì´í„° ì €ì¥
        df.to_csv(file_path, index=False, encoding='utf-8') # csv ìƒì„±

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    avatar = "https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg" if message["role"] == "user" else "ğŸŒ"

    with st.chat_message(message["role"], avatar = avatar):
        st.markdown(message["content"])
        
    

# React to user input
if user_input := st.chat_input("'LFON ê³„ì •ìƒì„±ì„ í•˜ê³ ì‹¶ì–´', 'ê²½ì˜ì •ë³´íŒ€ì— ëŒ€í•´ ì•Œë ¤ì¤˜'"):
    # Display user message in chat message container

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user", avatar="https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg"):
        #https://media.bunjang.co.kr/product/254293734_1_1708650713_w360.jpg
        #https://image.fmkorea.com/files/attach/new3/20230714/486616/1934106089/5966131270/9a6909c7b9bc74cd9c1418e330bb9a88.jpeg
        #https://mblogthumb-phinf.pstatic.net/MjAyMzEwMjFfNTMg/MDAxNjk3ODQ1NDI4NDk5.lz4l9LSm-znlfXxHK4ekPLjWmkAEfp-xMFTEuwy_8iwg.OHD1H2cwkXQ7nZtETfjgdELxhKWxElNXjlCyVpwOWLsg.PNG.ghkdwjdtka/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7_2023-10-20_224405.png?type=w800
        st.markdown(user_input)

          # Add user message to chat history
        

    with st.chat_message("assistant", avatar="ğŸŒ"):
        # response = st.write_stream(response_generator())
        prompt = create_prompt(user_input)
        response = st.write_stream(generate_response(prompt))

    st.session_state.messages.append({"role": "assistant", "content": response})

st.markdown("""
    <style>
    /*
   .stApp {
        background-color: lightblue;
    }   
    */    
    

    .stButton > button {
            color: white;
            background-color: black;
    }
                    
    
    h1 {
	    color: #FF3F00;
    }
            
    # .stTextInput {
    #         position:fixed;
    #         bottom: 3rem;

    # }
            
    .stChatMessage {
        justify-content: flex-start !important;
    }
            
    </style>
    """, unsafe_allow_html=True
)